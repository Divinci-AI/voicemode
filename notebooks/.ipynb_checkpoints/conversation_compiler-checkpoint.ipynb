{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Mode Conversation Compiler\n",
    "\n",
    "This notebook helps you explore, preview, and compile voice conversations saved by voice-mode.\n",
    "\n",
    "Audio files are stored in `~/.voicemode/audio/` with filenames like:\n",
    "- `YYYYMMDD_HHMMSS_SSS-tts.wav` (Text-to-Speech output)\n",
    "- `YYYYMMDD_HHMMSS_SSS-stt.wav` (Speech-to-Text input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install gradio pydub scipy numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import wave\n",
    "import json\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# Configuration\n",
    "AUDIO_DIR = Path.home() / \".voicemode\" / \"audio\"\n",
    "TRANSCRIPTIONS_DIR = Path.home() / \".voicemode\" / \"transcriptions\"\n",
    "OUTPUT_DIR = Path.home() / \".voicemode\" / \"compilations\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Audio directory: {AUDIO_DIR}\")\n",
    "print(f\"Transcriptions directory: {TRANSCRIPTIONS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_audio_filename(filename):\n",
    "    \"\"\"Parse timestamp and type from audio filename.\"\"\"\n",
    "    # Pattern: YYYYMMDD_HHMMSS_SSS-{tts|stt}.wav\n",
    "    match = re.match(r'(\\d{8})_(\\d{6})_(\\d{3})-(tts|stt)\\.wav', filename)\n",
    "    if match:\n",
    "        date_str, time_str, ms_str, audio_type = match.groups()\n",
    "        timestamp = datetime.strptime(f\"{date_str}_{time_str}\", \"%Y%m%d_%H%M%S\")\n",
    "        timestamp = timestamp.replace(microsecond=int(ms_str) * 1000)\n",
    "        return {\n",
    "            'filename': filename,\n",
    "            'timestamp': timestamp,\n",
    "            'type': audio_type,\n",
    "            'date': date_str,\n",
    "            'time': time_str\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def load_audio_files():\n",
    "    \"\"\"Load all audio files with metadata.\"\"\"\n",
    "    files = []\n",
    "    if AUDIO_DIR.exists():\n",
    "        for file in sorted(AUDIO_DIR.glob(\"*.wav\")):\n",
    "            parsed = parse_audio_filename(file.name)\n",
    "            if parsed:\n",
    "                parsed['path'] = file\n",
    "                # Try to load corresponding transcription\n",
    "                trans_file = TRANSCRIPTIONS_DIR / file.name.replace('.wav', '.txt')\n",
    "                if trans_file.exists():\n",
    "                    parsed['transcription'] = trans_file.read_text()\n",
    "                else:\n",
    "                    parsed['transcription'] = None\n",
    "                files.append(parsed)\n",
    "    return files\n",
    "\n",
    "# Load all audio files\n",
    "audio_files = load_audio_files()\n",
    "print(f\"Found {len(audio_files)} audio files\")\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "df = pd.DataFrame(audio_files)\n",
    "if not df.empty:\n",
    "    df = df.sort_values('timestamp')\n",
    "    print(f\"\\nDate range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"TTS files: {len(df[df['type'] == 'tts'])}\")\n",
    "    print(f\"STT files: {len(df[df['type'] == 'stt'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_conversations(df, time_window_seconds=120):\n",
    "    \"\"\"Match TTS and STT files into conversation pairs.\"\"\"\n",
    "    conversations = []\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df_sorted = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(df_sorted):\n",
    "        current = df_sorted.iloc[i]\n",
    "        \n",
    "        # Look for the next file within time window\n",
    "        j = i + 1\n",
    "        pair = None\n",
    "        \n",
    "        while j < len(df_sorted):\n",
    "            next_file = df_sorted.iloc[j]\n",
    "            time_diff = (next_file['timestamp'] - current['timestamp']).total_seconds()\n",
    "            \n",
    "            # If too far apart, break\n",
    "            if time_diff > time_window_seconds:\n",
    "                break\n",
    "            \n",
    "            # If different types, we found a pair\n",
    "            if current['type'] != next_file['type']:\n",
    "                pair = next_file\n",
    "                break\n",
    "            \n",
    "            j += 1\n",
    "        \n",
    "        if pair is not None:\n",
    "            # Create conversation entry\n",
    "            if current['type'] == 'tts':\n",
    "                tts, stt = current, pair\n",
    "            else:\n",
    "                stt, tts = current, pair\n",
    "            \n",
    "            conversations.append({\n",
    "                'timestamp': min(current['timestamp'], pair['timestamp']),\n",
    "                'tts_file': tts['filename'] if tts is not None else None,\n",
    "                'stt_file': stt['filename'] if stt is not None else None,\n",
    "                'tts_path': tts['path'] if tts is not None else None,\n",
    "                'stt_path': stt['path'] if stt is not None else None,\n",
    "                'tts_text': tts.get('transcription') if tts is not None else None,\n",
    "                'stt_text': stt.get('transcription') if stt is not None else None,\n",
    "                'gap_seconds': abs(time_diff) if pair else None\n",
    "            })\n",
    "            \n",
    "            # Skip the pair\n",
    "            i = max(i + 1, j + 1)\n",
    "        else:\n",
    "            # Unpaired file\n",
    "            conversations.append({\n",
    "                'timestamp': current['timestamp'],\n",
    "                'tts_file': current['filename'] if current['type'] == 'tts' else None,\n",
    "                'stt_file': current['filename'] if current['type'] == 'stt' else None,\n",
    "                'tts_path': current['path'] if current['type'] == 'tts' else None,\n",
    "                'stt_path': current['path'] if current['type'] == 'stt' else None,\n",
    "                'tts_text': current.get('transcription') if current['type'] == 'tts' else None,\n",
    "                'stt_text': current.get('transcription') if current['type'] == 'stt' else None,\n",
    "                'gap_seconds': None\n",
    "            })\n",
    "            i += 1\n",
    "    \n",
    "    return pd.DataFrame(conversations)\n",
    "\n",
    "# Match conversations\n",
    "if not df.empty:\n",
    "    conversations_df = match_conversations(df)\n",
    "    print(f\"\\nMatched {len(conversations_df)} conversation segments\")\n",
    "    print(f\"Paired conversations: {len(conversations_df[(conversations_df['tts_file'].notna()) & (conversations_df['stt_file'].notna())])}\")\n",
    "    print(f\"Unpaired TTS: {len(conversations_df[(conversations_df['tts_file'].notna()) & (conversations_df['stt_file'].isna())])}\")\n",
    "    print(f\"Unpaired STT: {len(conversations_df[(conversations_df['tts_file'].isna()) & (conversations_df['stt_file'].notna())])}\")\n",
    "else:\n",
    "    conversations_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_conversation(selected_indices, gap_duration_ms=500, normalize_volume=True):\n",
    "    \"\"\"Compile selected conversation segments into a single audio file.\"\"\"\n",
    "    if not selected_indices:\n",
    "        return None, \"No segments selected\"\n",
    "    \n",
    "    # Create silence gap\n",
    "    silence = AudioSegment.silent(duration=gap_duration_ms)\n",
    "    \n",
    "    # Start with empty audio\n",
    "    compiled = AudioSegment.empty()\n",
    "    \n",
    "    for idx in selected_indices:\n",
    "        if idx >= len(conversations_df):\n",
    "            continue\n",
    "            \n",
    "        row = conversations_df.iloc[idx]\n",
    "        \n",
    "        # Add STT (user speech) first if available\n",
    "        if row['stt_path'] and row['stt_path'].exists():\n",
    "            stt_audio = AudioSegment.from_wav(row['stt_path'])\n",
    "            if normalize_volume:\n",
    "                stt_audio = stt_audio.normalize()\n",
    "            compiled += stt_audio + silence\n",
    "        \n",
    "        # Then add TTS (assistant response)\n",
    "        if row['tts_path'] and row['tts_path'].exists():\n",
    "            tts_audio = AudioSegment.from_wav(row['tts_path'])\n",
    "            if normalize_volume:\n",
    "                tts_audio = tts_audio.normalize()\n",
    "            compiled += tts_audio + silence\n",
    "    \n",
    "    # Remove trailing silence\n",
    "    if len(compiled) > gap_duration_ms:\n",
    "        compiled = compiled[:-gap_duration_ms]\n",
    "    \n",
    "    # Export to file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = OUTPUT_DIR / f\"conversation_{timestamp}.wav\"\n",
    "    compiled.export(output_path, format=\"wav\")\n",
    "    \n",
    "    duration = len(compiled) / 1000.0  # Convert to seconds\n",
    "    return str(output_path), f\"Compiled {len(selected_indices)} segments into {duration:.1f} seconds of audio\\nSaved to: {output_path}\"\n",
    "\n",
    "def create_transcript(selected_indices):\n",
    "    \"\"\"Create a text transcript of selected conversation segments.\"\"\"\n",
    "    if not selected_indices:\n",
    "        return \"No segments selected\"\n",
    "    \n",
    "    transcript = []\n",
    "    \n",
    "    for idx in selected_indices:\n",
    "        if idx >= len(conversations_df):\n",
    "            continue\n",
    "            \n",
    "        row = conversations_df.iloc[idx]\n",
    "        timestamp = row['timestamp'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        transcript.append(f\"\\n[{timestamp}]\")\n",
    "        \n",
    "        if row['stt_text']:\n",
    "            transcript.append(f\"User: {row['stt_text']}\")\n",
    "        elif row['stt_file']:\n",
    "            transcript.append(f\"User: [Audio: {row['stt_file']}]\")\n",
    "        \n",
    "        if row['tts_text']:\n",
    "            transcript.append(f\"Assistant: {row['tts_text']}\")\n",
    "        elif row['tts_file']:\n",
    "            transcript.append(f\"Assistant: [Audio: {row['tts_file']}]\")\n",
    "    \n",
    "    return \"\\n\".join(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Voice Mode Conversation Compiler\") as demo:\n",
    "    gr.Markdown(\"# Voice Mode Conversation Compiler\")\n",
    "    gr.Markdown(\"Browse, preview, and compile voice conversations into audio files.\")\n",
    "    \n",
    "    with gr.Tab(\"Browse Conversations\"):\n",
    "        # Date filter\n",
    "        with gr.Row():\n",
    "            if not conversations_df.empty:\n",
    "                min_date = conversations_df['timestamp'].min().date()\n",
    "                max_date = conversations_df['timestamp'].max().date()\n",
    "            else:\n",
    "                min_date = max_date = datetime.now().date()\n",
    "            \n",
    "            date_filter = gr.Dropdown(\n",
    "                choices=[\"All\"] + sorted(df['date'].unique().tolist() if not df.empty else []),\n",
    "                value=\"All\",\n",
    "                label=\"Filter by Date\"\n",
    "            )\n",
    "            refresh_btn = gr.Button(\"Refresh Files\", variant=\"secondary\")\n",
    "        \n",
    "        # Conversation table\n",
    "        def format_conversations_table(date_filter=\"All\"):\n",
    "            filtered_df = conversations_df.copy()\n",
    "            \n",
    "            if date_filter != \"All\" and not filtered_df.empty:\n",
    "                filtered_df = filtered_df[filtered_df['timestamp'].dt.strftime('%Y%m%d') == date_filter]\n",
    "            \n",
    "            if filtered_df.empty:\n",
    "                return pd.DataFrame(columns=['Time', 'User Input', 'Assistant Response', 'Gap (s)'])\n",
    "            \n",
    "            display_df = pd.DataFrame({\n",
    "                'Index': filtered_df.index,\n",
    "                'Time': filtered_df['timestamp'].dt.strftime('%H:%M:%S'),\n",
    "                'User Input': filtered_df.apply(lambda x: x['stt_text'][:50] + '...' if x['stt_text'] and len(x['stt_text']) > 50 else (x['stt_text'] or '[Audio]' if x['stt_file'] else ''), axis=1),\n",
    "                'Assistant Response': filtered_df.apply(lambda x: x['tts_text'][:50] + '...' if x['tts_text'] and len(x['tts_text']) > 50 else (x['tts_text'] or '[Audio]' if x['tts_file'] else ''), axis=1),\n",
    "                'Gap (s)': filtered_df['gap_seconds'].apply(lambda x: f\"{x:.1f}\" if x else \"\")\n",
    "            })\n",
    "            return display_df\n",
    "        \n",
    "        conversations_table = gr.Dataframe(\n",
    "            value=format_conversations_table() if not conversations_df.empty else pd.DataFrame(),\n",
    "            label=\"Conversations\",\n",
    "            interactive=False\n",
    "        )\n",
    "        \n",
    "        # Selection\n",
    "        selected_indices = gr.State([])\n",
    "        selection_display = gr.Textbox(label=\"Selected Segments\", value=\"None selected\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            select_all_btn = gr.Button(\"Select All\", variant=\"secondary\")\n",
    "            clear_selection_btn = gr.Button(\"Clear Selection\", variant=\"secondary\")\n",
    "        \n",
    "        # Preview section\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                preview_index = gr.Number(label=\"Preview Index\", value=0, precision=0)\n",
    "                preview_audio_user = gr.Audio(label=\"User Audio\", type=\"filepath\")\n",
    "                preview_text_user = gr.Textbox(label=\"User Transcription\", lines=3)\n",
    "            \n",
    "            with gr.Column():\n",
    "                preview_btn = gr.Button(\"Preview\", variant=\"primary\")\n",
    "                preview_audio_assistant = gr.Audio(label=\"Assistant Audio\", type=\"filepath\")\n",
    "                preview_text_assistant = gr.Textbox(label=\"Assistant Transcription\", lines=3)\n",
    "    \n",
    "    with gr.Tab(\"Compile Audio\"):\n",
    "        gr.Markdown(\"### Compilation Settings\")\n",
    "        \n",
    "        gap_duration = gr.Slider(\n",
    "            minimum=0,\n",
    "            maximum=2000,\n",
    "            value=500,\n",
    "            step=100,\n",
    "            label=\"Gap Between Segments (ms)\"\n",
    "        )\n",
    "        \n",
    "        normalize_volume = gr.Checkbox(label=\"Normalize Volume\", value=True)\n",
    "        \n",
    "        compile_btn = gr.Button(\"Compile Selected Segments\", variant=\"primary\")\n",
    "        \n",
    "        compile_output = gr.Audio(label=\"Compiled Audio\", type=\"filepath\")\n",
    "        compile_status = gr.Textbox(label=\"Status\")\n",
    "        \n",
    "        gr.Markdown(\"### Transcript\")\n",
    "        transcript_output = gr.Textbox(label=\"Conversation Transcript\", lines=20)\n",
    "        export_transcript_btn = gr.Button(\"Export Transcript\", variant=\"secondary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def refresh_files_handler():\n",
    "        global audio_files, df, conversations_df\n",
    "        audio_files = load_audio_files()\n",
    "        df = pd.DataFrame(audio_files)\n",
    "        if not df.empty:\n",
    "            df = df.sort_values('timestamp')\n",
    "            conversations_df = match_conversations(df)\n",
    "        else:\n",
    "            conversations_df = pd.DataFrame()\n",
    "        return format_conversations_table()\n",
    "    \n",
    "    def filter_conversations(date_filter):\n",
    "        return format_conversations_table(date_filter)\n",
    "    \n",
    "    def preview_conversation(index):\n",
    "        if conversations_df.empty or index >= len(conversations_df) or index < 0:\n",
    "            return None, \"\", None, \"\"\n",
    "        \n",
    "        row = conversations_df.iloc[int(index)]\n",
    "        \n",
    "        stt_audio = str(row['stt_path']) if row['stt_path'] and row['stt_path'].exists() else None\n",
    "        stt_text = row['stt_text'] or \"\"\n",
    "        tts_audio = str(row['tts_path']) if row['tts_path'] and row['tts_path'].exists() else None\n",
    "        tts_text = row['tts_text'] or \"\"\n",
    "        \n",
    "        return stt_audio, stt_text, tts_audio, tts_text\n",
    "    \n",
    "    def select_all_segments(date_filter):\n",
    "        filtered_df = conversations_df.copy()\n",
    "        if date_filter != \"All\" and not filtered_df.empty:\n",
    "            filtered_df = filtered_df[filtered_df['timestamp'].dt.strftime('%Y%m%d') == date_filter]\n",
    "        \n",
    "        indices = list(filtered_df.index)\n",
    "        return indices, f\"Selected {len(indices)} segments\"\n",
    "    \n",
    "    def clear_selection():\n",
    "        return [], \"None selected\"\n",
    "    \n",
    "    def compile_handler(indices, gap_ms, normalize):\n",
    "        audio_path, status = compile_conversation(indices, gap_ms, normalize)\n",
    "        transcript = create_transcript(indices)\n",
    "        return audio_path, status, transcript\n",
    "    \n",
    "    def export_transcript(transcript):\n",
    "        if not transcript:\n",
    "            return \"No transcript to export\"\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = OUTPUT_DIR / f\"transcript_{timestamp}.txt\"\n",
    "        output_path.write_text(transcript)\n",
    "        return f\"Transcript exported to: {output_path}\"\n",
    "    \n",
    "    # Connect events\n",
    "    refresh_btn.click(refresh_files_handler, outputs=[conversations_table])\n",
    "    date_filter.change(filter_conversations, inputs=[date_filter], outputs=[conversations_table])\n",
    "    \n",
    "    preview_btn.click(\n",
    "        preview_conversation,\n",
    "        inputs=[preview_index],\n",
    "        outputs=[preview_audio_user, preview_text_user, preview_audio_assistant, preview_text_assistant]\n",
    "    )\n",
    "    \n",
    "    select_all_btn.click(\n",
    "        select_all_segments,\n",
    "        inputs=[date_filter],\n",
    "        outputs=[selected_indices, selection_display]\n",
    "    )\n",
    "    \n",
    "    clear_selection_btn.click(\n",
    "        clear_selection,\n",
    "        outputs=[selected_indices, selection_display]\n",
    "    )\n",
    "    \n",
    "    compile_btn.click(\n",
    "        compile_handler,\n",
    "        inputs=[selected_indices, gap_duration, normalize_volume],\n",
    "        outputs=[compile_output, compile_status, transcript_output]\n",
    "    )\n",
    "    \n",
    "    export_transcript_btn.click(\n",
    "        export_transcript,\n",
    "        inputs=[transcript_output],\n",
    "        outputs=[compile_status]\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "The cells below provide additional functionality for more advanced use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group conversations by session (large time gaps indicate new sessions)\n",
    "def identify_sessions(conversations_df, session_gap_minutes=30):\n",
    "    \"\"\"Group conversations into sessions based on time gaps.\"\"\"\n",
    "    if conversations_df.empty:\n",
    "        return conversations_df\n",
    "    \n",
    "    sessions = []\n",
    "    current_session = 0\n",
    "    last_timestamp = None\n",
    "    \n",
    "    for idx, row in conversations_df.iterrows():\n",
    "        if last_timestamp is None:\n",
    "            sessions.append(current_session)\n",
    "        else:\n",
    "            gap = (row['timestamp'] - last_timestamp).total_seconds() / 60\n",
    "            if gap > session_gap_minutes:\n",
    "                current_session += 1\n",
    "            sessions.append(current_session)\n",
    "        \n",
    "        last_timestamp = row['timestamp']\n",
    "    \n",
    "    conversations_df['session'] = sessions\n",
    "    return conversations_df\n",
    "\n",
    "# Add session information\n",
    "if not conversations_df.empty:\n",
    "    conversations_df = identify_sessions(conversations_df)\n",
    "    print(f\"\\nIdentified {conversations_df['session'].nunique()} conversation sessions\")\n",
    "    \n",
    "    # Show session summary\n",
    "    session_summary = conversations_df.groupby('session').agg({\n",
    "        'timestamp': ['min', 'max', 'count']\n",
    "    })\n",
    "    session_summary.columns = ['Start', 'End', 'Segments']\n",
    "    session_summary['Duration'] = session_summary['End'] - session_summary['Start']\n",
    "    print(\"\\nSession Summary:\")\n",
    "    print(session_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio analysis functions\n",
    "def analyze_audio_file(audio_path):\n",
    "    \"\"\"Analyze audio file properties.\"\"\"\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    \n",
    "    return {\n",
    "        'duration_seconds': len(audio) / 1000.0,\n",
    "        'channels': audio.channels,\n",
    "        'frame_rate': audio.frame_rate,\n",
    "        'sample_width': audio.sample_width,\n",
    "        'max_dBFS': audio.max_dBFS,\n",
    "        'rms': audio.rms,\n",
    "        'file_size_kb': audio_path.stat().st_size / 1024\n",
    "    }\n",
    "\n",
    "def remove_silence(audio_segment, min_silence_len=500, silence_thresh=-40, keep_silence=100):\n",
    "    \"\"\"Remove silence from audio segment.\"\"\"\n",
    "    chunks = split_on_silence(\n",
    "        audio_segment,\n",
    "        min_silence_len=min_silence_len,\n",
    "        silence_thresh=silence_thresh,\n",
    "        keep_silence=keep_silence\n",
    "    )\n",
    "    \n",
    "    # Combine chunks\n",
    "    if chunks:\n",
    "        return sum(chunks)\n",
    "    return audio_segment\n",
    "\n",
    "# Example: Analyze a specific audio file\n",
    "if not df.empty:\n",
    "    sample_file = df.iloc[0]['path']\n",
    "    print(f\"\\nAnalyzing {sample_file.name}:\")\n",
    "    analysis = analyze_audio_file(sample_file)\n",
    "    for key, value in analysis.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export functions for different formats\n",
    "def export_to_mp3(audio_segment, output_path, bitrate=\"192k\"):\n",
    "    \"\"\"Export audio to MP3 format.\"\"\"\n",
    "    audio_segment.export(output_path, format=\"mp3\", bitrate=bitrate)\n",
    "    return output_path\n",
    "\n",
    "def export_session_to_podcast(session_id, intro_text=None, outro_text=None):\n",
    "    \"\"\"Export a complete session as a podcast-style audio file.\"\"\"\n",
    "    session_df = conversations_df[conversations_df['session'] == session_id]\n",
    "    \n",
    "    if session_df.empty:\n",
    "        return None, \"No conversations in session\"\n",
    "    \n",
    "    # Compile the session\n",
    "    indices = list(session_df.index)\n",
    "    \n",
    "    # Create the main content\n",
    "    compiled = AudioSegment.empty()\n",
    "    silence = AudioSegment.silent(duration=800)  # Longer pauses for podcast style\n",
    "    \n",
    "    for idx in indices:\n",
    "        row = conversations_df.iloc[idx]\n",
    "        \n",
    "        # Add conversation\n",
    "        if row['stt_path'] and row['stt_path'].exists():\n",
    "            user_audio = AudioSegment.from_wav(row['stt_path'])\n",
    "            user_audio = user_audio.normalize()\n",
    "            # Reduce user audio volume slightly\n",
    "            user_audio = user_audio - 3\n",
    "            compiled += user_audio + silence\n",
    "        \n",
    "        if row['tts_path'] and row['tts_path'].exists():\n",
    "            assistant_audio = AudioSegment.from_wav(row['tts_path'])\n",
    "            assistant_audio = assistant_audio.normalize()\n",
    "            compiled += assistant_audio + silence\n",
    "    \n",
    "    # Add intro/outro if provided (would need TTS to generate these)\n",
    "    # This is a placeholder for the concept\n",
    "    \n",
    "    # Export as MP3\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = OUTPUT_DIR / f\"podcast_session_{session_id}_{timestamp}.mp3\"\n",
    "    \n",
    "    compiled.export(output_path, format=\"mp3\", bitrate=\"192k\")\n",
    "    \n",
    "    duration = len(compiled) / 1000.0 / 60.0  # Convert to minutes\n",
    "    return str(output_path), f\"Exported session {session_id} as podcast ({duration:.1f} minutes)\"\n",
    "\n",
    "# Example: Export first session as podcast\n",
    "if not conversations_df.empty and 'session' in conversations_df.columns:\n",
    "    first_session = conversations_df['session'].iloc[0]\n",
    "    print(f\"\\nExporting session {first_session} as podcast...\")\n",
    "    # Uncomment to actually export:\n",
    "    # output, status = export_session_to_podcast(first_session)\n",
    "    # print(status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}